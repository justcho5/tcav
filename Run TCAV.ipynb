{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running TCAV\n",
    "\n",
    "\n",
    "This notebook walks you through things you need to run TCAV. In high level, you need:\n",
    "\n",
    "1. **example images in each folder**\n",
    " * images for each concept\n",
    " * images for the class/labels of interest\n",
    " * random images that will be negative examples when learning CAVs (images that probably don't belong to any concepts)\n",
    "2. **model wrapper**: an instance of  ModelWrapper abstract class (in model.py). This tells TCAV class (tcav.py) how to communicate with your model (e.g., getting internal tensors)\n",
    "3. **act_generator**: an instance of ActivationGeneratorInterface that tells TCAV class how to load example data and how to get activations from the model\n",
    "\n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "    pip install the tcav and tensorflow packages (or tensorflow-gpu if using GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justina/anaconda3/envs/py37_tf113/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/justina/anaconda3/envs/py37_tf113/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/justina/anaconda3/envs/py37_tf113/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/justina/anaconda3/envs/py37_tf113/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/justina/anaconda3/envs/py37_tf113/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/justina/anaconda3/envs/py37_tf113/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tcav.activation_generator as act_gen\n",
    "import tcav.cav as cav\n",
    "import tcav.model  as model\n",
    "import tcav.tcav as tcav\n",
    "import tcav.utils as utils\n",
    "import tcav.utils_plot as utils_plot # utils_plot requires matplotlib\n",
    "import os \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Store concept and target class images to local folders\n",
    "\n",
    "and tell TCAV where they are.\n",
    "\n",
    "**source_dir**: where images of concepts, target class and random images (negative samples when learning CAVs) live. Each should be a sub-folder within this directory.\n",
    "\n",
    "Note that random image directories can be in any name. In this example, we are using `random500_0`, `random500_1`,.. for an arbitrary reason. \n",
    "\n",
    "\n",
    "You need roughly 50-200 images per concept and target class (10-20 pictures also tend to work, but 200 is pretty safe).\n",
    "\n",
    "\n",
    "**cav_dir**: directory to store CAVs (`None` if you don't want to store)\n",
    "\n",
    "**target, concept**: names of the target class (that you want to investigate) and concepts (strings) - these are folder names in source_dir\n",
    "\n",
    "**bottlenecks**: list of bottleneck names (intermediate layers in your model) that you want to use for TCAV. These names are defined in the model wrapper below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder prefix \n",
    "# Mac\n",
    "prefix = '/Users/justina/Documents/EPFL/thesis/project/hnsc/tcav'\n",
    "# Ubuntu\n",
    "# prefix = '/usr/local/google/home/beenkim'\n",
    "\n",
    "print ('REMEMBER TO UPDATE FOLDER PREFIX!')\n",
    "\n",
    "# This is the name of your model wrapper (InceptionV3 and GoogleNet are provided in model.py)\n",
    "model_to_run = 'GoogleNet'  \n",
    "user = 'justina'\n",
    "# the name of the parent directory that results are stored (only if you want to cache)\n",
    "project_name = 'tcav_class_test'\n",
    "working_dir = prefix + \"/tmp/\" + user + '/' + project_name\n",
    "print(working_dir)\n",
    "# where activations are stored (only if your act_gen_wrapper does so)\n",
    "activation_dir =  working_dir+ '/activations/'\n",
    "print(activation_dir)\n",
    "# where CAVs are stored. \n",
    "# You can say None if you don't wish to store any.\n",
    "cav_dir = working_dir + '/cavs/'\n",
    "print(cav_dir)\n",
    "# where the images live. \n",
    "source_dir = prefix + \"/source_dir/\"\n",
    "print(source_dir)\n",
    "bottlenecks = [ 'mixed4c']  # @param \n",
    "      \n",
    "utils.make_dir_if_not_exists(activation_dir)\n",
    "utils.make_dir_if_not_exists(working_dir)\n",
    "utils.make_dir_if_not_exists(cav_dir)\n",
    "\n",
    "# this is a regularizer penalty parameter for linear classifier to get CAVs. \n",
    "alphas = [0.1]   \n",
    "\n",
    "target = 'zebra'  \n",
    "concepts = [\"dotted\",\"striped\",\"zigzagged\"]   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Write your model wrapper\n",
    "\n",
    "Next step is to tell TCAV how to communicate with your model. See `model.GoolgeNetWrapper_public ` for details.\n",
    "\n",
    "You can define a subclass of ModelWrapper abstract class to do this. Let me walk you thru what each function does (tho they are pretty self-explanatory).  This wrapper includes a lot of the functions that you already have, for example, `get_prediction`.\n",
    "\n",
    "### 1. Tensors from the graph: bottleneck tensors and ends\n",
    "First, store your bottleneck tensors in `self.bottlenecks_tensors` as a dictionary. You only need bottlenecks that you are interested in running TCAV with. Similarly, fill in `self.ends` dictionary with `input`, `logit` and `prediction` tensors.\n",
    "\n",
    "### 2. Define loss\n",
    "Get your loss tensor, and assigned it to `self.loss`. This is what TCAV uses to take directional derivatives. \n",
    "\n",
    "While doing so, you would also want to set \n",
    "```python\n",
    "self.y_input \n",
    "```\n",
    "this simply is a tensorflow place holder for the target index in the logit layer (e.g., 0 index for a dog, 1 for a cat).\n",
    "For multi-class classification, typically something like this works:\n",
    "\n",
    "```python\n",
    "self.y_input = tf.placeholder(tf.int64, shape=[None])\n",
    "```\n",
    "\n",
    "For example, for a multiclass classifier, something like below would work. \n",
    "\n",
    "```python\n",
    "    # Construct gradient ops.\n",
    "    with g.as_default():\n",
    "      self.y_input = tf.placeholder(tf.int64, shape=[None])\n",
    "\n",
    "      self.pred = tf.expand_dims(self.ends['prediction'][0], 0)\n",
    "\n",
    "      self.loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "              labels=tf.one_hot(self.y_input, len(self.labels)),\n",
    "              logits=self.pred))\n",
    "    self._make_gradient_tensors()\n",
    "```\n",
    "\n",
    "### 3. Call _make_gradient_tensors in __init__() of your wrapper\n",
    "```python\n",
    "_make_gradient_tensors()  \n",
    "```\n",
    "does what you expect - given the loss and bottleneck tensors defined above, it adds gradient tensors.\n",
    "\n",
    "### 4. Fill in labels, image shapes and a model name.\n",
    "Get the mapping from labels (strings) to indice in the logit layer (int) in a dictionary format.\n",
    "\n",
    "```python\n",
    "def id_to_label(self, idx)\n",
    "def label_to_id(self, label)\n",
    "```\n",
    "\n",
    "Set your input image shape at  `self.image_shape`\n",
    "\n",
    "\n",
    "Set your model name to `self.model_name`\n",
    "\n",
    "You are done with writing the model wrapper! I wrote two model wrapers, InceptionV3 and Googlenet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**sess**: a tensorflow session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOTTLENECK: {'add/add': <tf.Tensor 'import/xception_2/add/add:0' shape=(?, ?, ?, 128) dtype=float32>, 'add_1/add': <tf.Tensor 'import/xception_2/add_1/add:0' shape=(?, ?, ?, 256) dtype=float32>, 'add_2/add': <tf.Tensor 'import/xception_2/add_2/add:0' shape=(?, ?, ?, 728) dtype=float32>, 'add_3/add': <tf.Tensor 'import/xception_2/add_3/add:0' shape=(?, ?, ?, 728) dtype=float32>, 'add_4/add': <tf.Tensor 'import/xception_2/add_4/add:0' shape=(?, ?, ?, 728) dtype=float32>, 'add_5/add': <tf.Tensor 'import/xception_2/add_5/add:0' shape=(?, ?, ?, 728) dtype=float32>, 'add_6/add': <tf.Tensor 'import/xception_2/add_6/add:0' shape=(?, ?, ?, 728) dtype=float32>, 'add_7/add': <tf.Tensor 'import/xception_2/add_7/add:0' shape=(?, ?, ?, 728) dtype=float32>, 'add_8/add': <tf.Tensor 'import/xception_2/add_8/add:0' shape=(?, ?, ?, 728) dtype=float32>, 'add_9/add': <tf.Tensor 'import/xception_2/add_9/add:0' shape=(?, ?, ?, 728) dtype=float32>, 'add_10/add': <tf.Tensor 'import/xception_2/add_10/add:0' shape=(?, ?, ?, 728) dtype=float32>, 'add_11/add': <tf.Tensor 'import/xception_2/add_11/add:0' shape=(?, ?, ?, 1024) dtype=float32>}\n",
      "WARNING:tensorflow:From /Users/justina/Documents/EPFL/thesis/project/hnsc/tcav/tcav/model.py:273: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /Users/justina/anaconda3/envs/py37_tf113/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/justina/anaconda3/envs/py37_tf113/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "sess = utils.create_session()\n",
    "\n",
    "# GRAPH_PATH is where the trained model is stored.\n",
    "# GRAPH_PATH =  prefix + \"/tensorflow_inception_graph.pb\"\n",
    "GRAPH_PATH = \"./frozen_model.pb\"\n",
    "# LABEL_PATH is where the labels are stored. Each line contains one class, and they are ordered with respect to their index in \n",
    "# the logit layer. (yes, id_to_label function in the model wrapper reads from this file.)\n",
    "# For example, imagenet_comp_graph_label_strings.txt looks like:\n",
    "# dummy                                                                                      \n",
    "# kit fox\n",
    "# English setter\n",
    "# Siberian husky ...\n",
    "\n",
    "# LABEL_PATH = prefix + \"/imagenet_comp_graph_label_strings.txt\"\n",
    "LABEL_PATH = \"./model/labels.txt\"\n",
    "\n",
    "mymodel = model.XceptionHPVWrapper(sess,\n",
    "                                        GRAPH_PATH,\n",
    "                                        LABEL_PATH)\n",
    "\n",
    "# mymodel = model.GoolgeNetWrapper_public(sess,\n",
    "#                                         GRAPH_PATH,\n",
    "#                                         LABEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Implement a class that returns activations (maybe with caching!)\n",
    "\n",
    "Lastly, you will implement a class of the ActivationGenerationInterface which TCAV uses to load example data for a given concept or target, call into your model wrapper and return activations. I pulled out this logic outside of mymodel because this step often takes the longest. By making it modular, you can cache your activations and/or parallelize your computations, as I have done in `ActivationGeneratorBase.process_and_load_activations` in `activation_generator.py`.\n",
    "\n",
    "\n",
    "The `process_and_load_activations` method of the activation generator must return a dictionary of activations that has concept or target name as  a first key, and the bottleneck name as a second key. So something like:\n",
    "\n",
    "```python\n",
    "{concept1: {bottleneck1: [[0.2, 0.1, ....]]},\n",
    "concept2: {bottleneck1: [[0.1, 0.02, ....]]},\n",
    "target1: {bottleneck1: [[0.02, 0.99, ....]]}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_generator = act_gen.ImageActivationGenerator(mymodel, source_dir, activation_dir, max_examples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You are ready to run TCAV!\n",
    "\n",
    "Let's do it.\n",
    "\n",
    "**num_random_exp**: number of experiments to confirm meaningful concept direction. TCAV will search for this many folders named `random500_0`, `random500_1`, etc. You can alternatively set the `random_concepts` keyword to be a list of folders of random concepts. Run at least 10-20 for meaningful tests. \n",
    "\n",
    "**random_counterpart**: as well as the above, you can optionally supply a single folder with random images as the \"positive set\" for statistical testing. Reduces computation time at the cost of less reliable random TCAV scores. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(0)\n",
    "## only running num_random_exp = 10 to save some time. The paper number are reported for 500 random runs. \n",
    "mytcav = tcav.TCAV(sess,\n",
    "                   target,\n",
    "                   concepts,\n",
    "                   bottlenecks,\n",
    "                   act_generator,\n",
    "                   alphas,\n",
    "                   cav_dir=cav_dir,\n",
    "                   num_random_exp=10)\n",
    "print ('This may take a while... Go get coffee!')\n",
    "results = mytcav.run(run_parallel=False)\n",
    "print ('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "utils_plot.plot_results(results, num_random_exp=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.applications.inception_v3 import InceptionV3\n",
    "# from keras.models import Model\n",
    "# from keras import backend as K\n",
    "# base_model = InceptionV3(weights='imagenet')\n",
    "\n",
    "# base_model.layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "sess = utils.create_session()\n",
    "\n",
    "f = gfile.FastGFile(\"./tensorflow_inception_graph.pb\", 'rb')\n",
    "graph_def = tf.GraphDef()\n",
    "# Parses a serialized binary message into the current message.\n",
    "graph_def.ParseFromString(f.read())\n",
    "f.close()\n",
    "\n",
    "sess.graph.as_default()\n",
    "# Import a serialized TensorFlow `GraphDef` protocol buffer\n",
    "# and place into the current default `Graph`.\n",
    "tf.import_graph_def(graph_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n",
    "LOGDIR='./logs/tests/2/'\n",
    "train_writer = tf.summary.FileWriter(LOGDIR)\n",
    "train_writer.add_graph(sess.graph)\n",
    "# graph.get_operations()\n",
    "for op in graph.get_operations():\n",
    "    print(\"name: {}\".format(op.name))\n",
    "    print(\"values: {}\".format(op.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import ttest_ind\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tcav.tcav_results.results_pb2 import Result, Results\n",
    "# import os\n",
    "# import keras\n",
    "# from keras import backend as K\n",
    "# tf.keras.backend.set_learning_phase(0)\n",
    "\n",
    "\n",
    "# def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "#     \"\"\"\n",
    "#     Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "#     Creates a new computation graph where variable nodes are replaced by\n",
    "#     constants taking their current value in the session. The new graph will be\n",
    "#     pruned so subgraphs that are not necessary to compute the requested\n",
    "#     outputs are removed.\n",
    "#     @param session The TensorFlow session to be frozen.\n",
    "#     @param keep_var_names A list of variable names that should not be frozen,\n",
    "#                           or None to freeze all the variables in the graph.\n",
    "#     @param output_names Names of the relevant graph outputs.\n",
    "#     @param clear_devices Remove the device directives from the graph for better portability.\n",
    "#     @return The frozen graph definition.\n",
    "#     \"\"\"\n",
    "#     from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "#     graph = session.graph\n",
    "#     with graph.as_default():\n",
    "#         freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "#         output_names = output_names or []\n",
    "#         output_names += [v.op.name for v in tf.global_variables()]\n",
    "#         # Graph -> GraphDef ProtoBuf\n",
    "#         input_graph_def = graph.as_graph_def()\n",
    "#         if clear_devices:\n",
    "#             for node in input_graph_def.node:\n",
    "#                 node.device = \"\"\n",
    "#         frozen_graph = convert_variables_to_constants(session, input_graph_def,\n",
    "#                                                       output_names, freeze_var_names)\n",
    "#         return frozen_graph\n",
    "\n",
    "# def write_pb(keras_model_path):\n",
    "#     model = tf.keras.models.load_model(keras_model_path, compile=False)\n",
    "#     model.compile(loss='sparse_categorical_crossentropy',\n",
    "#                 optimizer=tf.keras.optimizers.Adam())\n",
    "#     frozen_graph = freeze_session(K.get_session(),output_names=[out.op.name for out in model.outputs])\n",
    "#     tf.train.write_graph(frozen_graph, \"model\", \"hpv_xception.pb\", as_text=False)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"/Users/justina/Documents/EPFL/thesis/project/hnsc/trained_model.h5\"\n",
    "# write_pb(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import graph_io\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "\n",
    "def freeze_graph(graph, session, output):\n",
    "    with graph.as_default():\n",
    "        graphdef_inf = tf.graph_util.remove_training_nodes(graph.as_graph_def())\n",
    "        graphdef_frozen = tf.graph_util.convert_variables_to_constants(session, graphdef_inf, output)\n",
    "        graph_io.write_graph(graphdef_frozen, \".\", \"frozen_model.pb\", as_text=True)\n",
    "\n",
    "tf.keras.backend.set_learning_phase(0) # this line most important\n",
    "\n",
    "keras_model_path = \"/Users/justina/Documents/EPFL/thesis/project/hnsc/trained_model.h5\"\n",
    "\n",
    "base_model = tf.keras.models.load_model(keras_model_path, compile=False)\n",
    "base_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Adam())\n",
    "session = tf.keras.backend.get_session()\n",
    "\n",
    "INPUT_NODE = base_model.inputs[0].op.name\n",
    "OUTPUT_NODE = base_model.outputs[0].op.name\n",
    "freeze_graph(session.graph, session, [out.op.name for out in base_model.outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-565367278fb4>:4: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justina/anaconda3/envs/py37_tf113/lib/python3.7/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'import/block1_conv1_2/kernel' type=Const>,\n",
       " <tf.Operation 'import/block1_conv1_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block1_conv1_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block1_conv1_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block1_conv1_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block1_conv2_2/kernel' type=Const>,\n",
       " <tf.Operation 'import/block1_conv2_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block1_conv2_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block1_conv2_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block1_conv2_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block2_sepconv1_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block2_sepconv1_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block2_sepconv1_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block2_sepconv1_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block2_sepconv1_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block2_sepconv1_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block2_sepconv2_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block2_sepconv2_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block2_sepconv2_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block2_sepconv2_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block2_sepconv2_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block2_sepconv2_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/conv2d_96/kernel' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_96/gamma' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_96/beta' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_96/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_96/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block3_sepconv1_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block3_sepconv1_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block3_sepconv1_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block3_sepconv1_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block3_sepconv1_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block3_sepconv1_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block3_sepconv2_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block3_sepconv2_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block3_sepconv2_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block3_sepconv2_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block3_sepconv2_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block3_sepconv2_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/conv2d_1_3/kernel' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_1_3/gamma' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_1_3/beta' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_1_3/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_1_3/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block4_sepconv1_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block4_sepconv1_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block4_sepconv1_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block4_sepconv1_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block4_sepconv1_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block4_sepconv1_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block4_sepconv2_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block4_sepconv2_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block4_sepconv2_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block4_sepconv2_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block4_sepconv2_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block4_sepconv2_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/conv2d_2_3/kernel' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_2_3/gamma' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_2_3/beta' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_2_3/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_2_3/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block5_sepconv1_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block5_sepconv1_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block5_sepconv1_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block5_sepconv1_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block5_sepconv1_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block5_sepconv1_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block5_sepconv2_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block5_sepconv2_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block5_sepconv2_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block5_sepconv2_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block5_sepconv2_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block5_sepconv2_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block5_sepconv3_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block5_sepconv3_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block5_sepconv3_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block5_sepconv3_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block5_sepconv3_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block5_sepconv3_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block6_sepconv1_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block6_sepconv1_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block6_sepconv1_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block6_sepconv1_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block6_sepconv1_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block6_sepconv1_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block6_sepconv2_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block6_sepconv2_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block6_sepconv2_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block6_sepconv2_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block6_sepconv2_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block6_sepconv2_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block6_sepconv3_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block6_sepconv3_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block6_sepconv3_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block6_sepconv3_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block6_sepconv3_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block6_sepconv3_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block7_sepconv1_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block7_sepconv1_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block7_sepconv1_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block7_sepconv1_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block7_sepconv1_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block7_sepconv1_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block7_sepconv2_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block7_sepconv2_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block7_sepconv2_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block7_sepconv2_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block7_sepconv2_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block7_sepconv2_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block7_sepconv3_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block7_sepconv3_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block7_sepconv3_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block7_sepconv3_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block7_sepconv3_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block7_sepconv3_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block8_sepconv1_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block8_sepconv1_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block8_sepconv1_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block8_sepconv1_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block8_sepconv1_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block8_sepconv1_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block8_sepconv2_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block8_sepconv2_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block8_sepconv2_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block8_sepconv2_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block8_sepconv2_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block8_sepconv2_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block8_sepconv3_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block8_sepconv3_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block8_sepconv3_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block8_sepconv3_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block8_sepconv3_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block8_sepconv3_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block9_sepconv1_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block9_sepconv1_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block9_sepconv1_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block9_sepconv1_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block9_sepconv1_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block9_sepconv1_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block9_sepconv2_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block9_sepconv2_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block9_sepconv2_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block9_sepconv2_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block9_sepconv2_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block9_sepconv2_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block9_sepconv3_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block9_sepconv3_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block9_sepconv3_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block9_sepconv3_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block9_sepconv3_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block9_sepconv3_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block10_sepconv1_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block10_sepconv1_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block10_sepconv1_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block10_sepconv1_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block10_sepconv1_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block10_sepconv1_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block10_sepconv2_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block10_sepconv2_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block10_sepconv2_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block10_sepconv2_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block10_sepconv2_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block10_sepconv2_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block10_sepconv3_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block10_sepconv3_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block10_sepconv3_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block10_sepconv3_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block10_sepconv3_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block10_sepconv3_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block11_sepconv1_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block11_sepconv1_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block11_sepconv1_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block11_sepconv1_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block11_sepconv1_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block11_sepconv1_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block11_sepconv2_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block11_sepconv2_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block11_sepconv2_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block11_sepconv2_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block11_sepconv2_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block11_sepconv2_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block11_sepconv3_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block11_sepconv3_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block11_sepconv3_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block11_sepconv3_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block11_sepconv3_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block11_sepconv3_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block12_sepconv1_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block12_sepconv1_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block12_sepconv1_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block12_sepconv1_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block12_sepconv1_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block12_sepconv1_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block12_sepconv2_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block12_sepconv2_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block12_sepconv2_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block12_sepconv2_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block12_sepconv2_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block12_sepconv2_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block12_sepconv3_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block12_sepconv3_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block12_sepconv3_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block12_sepconv3_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block12_sepconv3_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block12_sepconv3_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block13_sepconv1_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block13_sepconv1_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block13_sepconv1_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block13_sepconv1_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block13_sepconv1_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block13_sepconv1_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block13_sepconv2_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block13_sepconv2_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block13_sepconv2_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block13_sepconv2_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block13_sepconv2_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block13_sepconv2_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/conv2d_3_3/kernel' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_3_3/gamma' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_3_3/beta' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_3_3/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_3_3/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block14_sepconv1_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block14_sepconv1_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block14_sepconv1_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block14_sepconv1_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block14_sepconv1_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block14_sepconv1_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/block14_sepconv2_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block14_sepconv2_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/block14_sepconv2_bn_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/block14_sepconv2_bn_2/beta' type=Const>,\n",
       " <tf.Operation 'import/block14_sepconv2_bn_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/block14_sepconv2_bn_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/xception_input_2' type=Placeholder>,\n",
       " <tf.Operation 'import/xception_2/block1_conv1/Conv2D/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block1_conv1/Conv2D' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block1_conv1_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block1_conv1_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block1_conv1_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block1_conv1_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block1_conv1_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block1_conv1_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block1_conv2/Conv2D/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block1_conv2/Conv2D' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block1_conv2_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block1_conv2_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block1_conv2_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block1_conv2_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block1_conv2_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block1_conv2_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block2_sepconv1/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block2_sepconv1/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block2_sepconv1/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block2_sepconv1/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block2_sepconv1_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block2_sepconv1_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block2_sepconv1_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block2_sepconv1_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block2_sepconv1_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block2_sepconv2_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block2_sepconv2/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block2_sepconv2/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block2_sepconv2/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block2_sepconv2/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/conv2d/Conv2D/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/conv2d/Conv2D' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block2_sepconv2_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block2_sepconv2_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block2_sepconv2_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block2_sepconv2_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block2_sepconv2_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block2_pool/MaxPool' type=MaxPool>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/add/add' type=Add>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv1_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv1/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv1/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv1/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv1/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv1_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv1_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv1_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv1_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv1_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv2_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv2/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv2/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv2/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv2/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/conv2d_1/Conv2D/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/conv2d_1/Conv2D' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv2_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv2_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv2_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv2_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block3_sepconv2_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block3_pool/MaxPool' type=MaxPool>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization_1/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization_1/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization_1/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization_1/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization_1/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/add_1/add' type=Add>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv1_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv1/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv1/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv1/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv1/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv1_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv1_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv1_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv1_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv1_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv2_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv2/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv2/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv2/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv2/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/conv2d_2/Conv2D/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/conv2d_2/Conv2D' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv2_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv2_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv2_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv2_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block4_sepconv2_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block4_pool/MaxPool' type=MaxPool>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization_2/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization_2/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization_2/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization_2/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization_2/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/add_2/add' type=Add>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv1_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv1/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv1/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv1/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv1/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv1_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv1_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv1_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv1_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv1_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv2_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv2/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv2/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv2/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv2/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv2_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv2_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv2_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv2_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv2_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv3_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv3/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv3/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv3/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv3/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv3_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv3_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv3_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv3_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block5_sepconv3_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/add_3/add' type=Add>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv1_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv1/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv1/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv1/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv1/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv1_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv1_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv1_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv1_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv1_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv2_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv2/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv2/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv2/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv2/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv2_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv2_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv2_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv2_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv2_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv3_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv3/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv3/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv3/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv3/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv3_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv3_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv3_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv3_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block6_sepconv3_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/add_4/add' type=Add>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv1_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv1/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv1/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv1/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv1/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv1_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv1_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv1_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv1_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv1_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv2_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv2/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv2/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv2/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv2/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv2_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv2_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv2_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv2_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv2_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv3_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv3/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv3/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv3/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv3/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv3_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv3_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv3_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv3_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block7_sepconv3_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/add_5/add' type=Add>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv1_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv1/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv1/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv1/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv1/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv1_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv1_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv1_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv1_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv1_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv2_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv2/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv2/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv2/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv2/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv2_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv2_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv2_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv2_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv2_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv3_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv3/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv3/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv3/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv3/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv3_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv3_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv3_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv3_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block8_sepconv3_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/add_6/add' type=Add>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv1_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv1/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv1/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv1/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv1/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv1_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv1_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv1_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv1_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv1_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv2_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv2/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv2/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv2/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv2/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv2_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv2_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv2_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv2_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv2_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv3_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv3/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv3/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv3/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv3/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv3_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv3_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv3_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv3_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block9_sepconv3_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/add_7/add' type=Add>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv1_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv1/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv1/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv1/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv1/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv1_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv1_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv1_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv1_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv1_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv2_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv2/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv2/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv2/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv2/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv2_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv2_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv2_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv2_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv2_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv3_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv3/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv3/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv3/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv3/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv3_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv3_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv3_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv3_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block10_sepconv3_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/add_8/add' type=Add>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv1_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv1/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv1/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv1/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv1/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv1_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv1_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv1_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv1_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv1_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv2_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv2/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv2/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv2/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv2/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv2_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv2_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv2_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv2_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv2_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv3_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv3/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv3/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv3/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv3/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv3_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv3_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv3_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv3_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block11_sepconv3_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/add_9/add' type=Add>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv1_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv1/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv1/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv1/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv1/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv1_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv1_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv1_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv1_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv1_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv2_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv2/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv2/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv2/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv2/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv2_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv2_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv2_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv2_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv2_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv3_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv3/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv3/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv3/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv3/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv3_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv3_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv3_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv3_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block12_sepconv3_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/add_10/add' type=Add>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv1_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv1/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv1/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv1/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv1/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv1_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv1_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv1_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv1_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv1_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv2_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv2/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv2/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv2/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv2/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/conv2d_3/Conv2D/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/conv2d_3/Conv2D' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv2_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv2_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv2_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv2_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block13_sepconv2_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block13_pool/MaxPool' type=MaxPool>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization_3/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization_3/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization_3/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization_3/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/batch_normalization_3/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/add_11/add' type=Add>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv1/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv1/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv1/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv1/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv1_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv1_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv1_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv1_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv1_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv1_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv2/separable_conv2d/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv2/separable_conv2d/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv2/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv2/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv2_bn/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv2_bn/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv2_bn/FusedBatchNorm/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv2_bn/FusedBatchNorm/ReadVariableOp_1' type=Identity>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv2_bn/FusedBatchNorm' type=FusedBatchNorm>,\n",
       " <tf.Operation 'import/xception_2/block14_sepconv2_act/Relu' type=Relu>,\n",
       " <tf.Operation 'import/xception_2/global_max_pooling2d/Max/reduction_indices' type=Const>,\n",
       " <tf.Operation 'import/xception_2/global_max_pooling2d/Max' type=Max>,\n",
       " <tf.Operation 'import/dense_3/kernel' type=Const>,\n",
       " <tf.Operation 'import/dense_3/bias' type=Const>,\n",
       " <tf.Operation 'import/dense_3/MatMul/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/dense_3/MatMul' type=MatMul>,\n",
       " <tf.Operation 'import/dense_3/BiasAdd/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/dense_3/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'import/dense_3/Relu' type=Relu>,\n",
       " <tf.Operation 'import/dense_1_2/kernel' type=Const>,\n",
       " <tf.Operation 'import/dense_1_2/bias' type=Const>,\n",
       " <tf.Operation 'import/dense_1_2/MatMul/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/dense_1_2/MatMul' type=MatMul>,\n",
       " <tf.Operation 'import/dense_1_2/BiasAdd/ReadVariableOp' type=Identity>,\n",
       " <tf.Operation 'import/dense_1_2/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'import/dense_1_2/Softmax' type=Softmax>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.platform import gfile\n",
    "sess = utils.create_session()\n",
    "\n",
    "f = gfile.FastGFile(\"./model/frozen_model.pb\", 'rb')\n",
    "graph_def = tf.GraphDef()\n",
    "# Parses a serialized binary message into the current message.\n",
    "graph_def.ParseFromString(f.read())\n",
    "print(graph_def.node)\n",
    "f.close()\n",
    "\n",
    "sess.graph.as_default()\n",
    "# Import a serialized TensorFlow `GraphDef` protocol buffer\n",
    "# and place into the current default `Graph`.\n",
    "tf.import_graph_def(graph_def)\n",
    "g = tf.get_default_graph()\n",
    "g.get_operations()\n",
    "\n",
    "\n",
    "-> 323     graph_def = tf.GraphDef.FromString(tf.gfile.Open(saved_path, 'rb').read())\n",
    "    324 \n",
    "    325     with tf.name_scope(scope) as sc:\n",
    "\n",
    "DecodeError: Error parsing message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_def.node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model.layers\n",
    "\n",
    "g.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for op in g.get_operations():\n",
    "    print(\"name: {}\".format(op.name))\n",
    "    print(\"values: {}\".format(op.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR='./logs/tests/1/'\n",
    "train_writer = tf.summary.FileWriter(LOGDIR)\n",
    "train_writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/tests/1/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/tests/2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37_tf113]",
   "language": "python",
   "name": "conda-env-py37_tf113-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
